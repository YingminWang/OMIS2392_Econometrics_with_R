stargazer(poisson1c,
apply.coef = exp, t.auto = F, p.auto = F,
se=HWrobstder,
title='Regression Results', type='text',
column.labels = "IRRs with HW-Robust SE",
df=FALSE, digits=3, star.cutoffs = c(0.05,0.01,0.001))
## visualize output
meffects <- ggpredict(poisson1, terms=c('facebookvisit'))
ggplot(meffects,aes(x, predicted)) + geom_point(size=3, colour="maroon") + geom_line() +
xlab("facebook visit") + ylab("Predicted purchase") +
scale_x_continuous(breaks=c(0,1), labels=c("with facebook visit", "without facebook visit"))
meffects
# out of sample prediction   ??? mean(gender)?    ??? the purpose of compare out of sample prediction with meffects?
newdata <- data.frame(income=mean(mydata$income), gender=mean(mydata$gender),
distance = mean(mydata$distance), cust_age=mean(mydata$cust_age),
numofposts=mean(mydata$numofposts), facebookvisit = c(0,1))
# IRRs          ?? The same as Poission?
stargazer(negbin1,
apply.coef=exp, t.auto=F, p.auto=F,
title='Regression Results', type='text',
column.labels = 'IRRs',
df=FALSE, digits=4, star.cutoffs = c(0.05,0.01,0.001))
stargazer(negbin1, negbin1,
se=list(NULL, HWrobstder),
title="Regression Results", type="text",
column.labels=c("Normal SE", "HW-Robust SE"),
df=FALSE, digits=4, star.cutoffs = c(0.05,0.01,0.001))
View(newdata)
View(mydata)
#==========================================================
## Compare with linear model
#==========================================================
# different
m1
# 2. dependent variable: <purchase>: number of purchases made by customer i
ggplot(mydata, aes(x=purchase)) + geom_histogram(color='green')
head(mydata)
stargazer(mydata, type='text', title='descriptive analysis', digits=2, median=TRUE, iqr=TRUE)
model2<- ivreg(lnpurchase ~ facebookvisit + income+ gender + distance + cust_age + numofposts + numoffriends + PublicProfile, data=mydata)
model2<- ivreg(lnpurchase ~ facebookvisit + income+ gender + distance + cust_age + numofposts + numoffriends + PublicProfile, data=mydata)
stargazer(model2
title="Regression Results", type="text",
column.labels=c( "Model-2"),
df=FALSE, digits=2, star.cutoffs = c(0.05,0.01,0.001))
stargazer( model2,
title="Regression Results", type="text",
column.labels=c("Model-2"),
df=FALSE, digits=2, star.cutoffs = c(0.05,0.01,0.001))
summary(model2,diagnostics = TRUE)
summary(model2,diagnostics = TRUE)
library(stargazer)
library(gdata)
library(ggplot2)
library(psych)
library(ggeffects)
library(QuantPsyc)
library(VIF)
library(usdm)
library(lmtest)
library(multiwayvcov)
library(sandwich)
library(foreign)
library(AER)
summary(model2,diagnostics = TRUE)
# question how to test indogeneity?
m2 <- ivreg(lnpurchase ~ facebookvisit + income+ gender + distance + cust_age + numofposts|numoffriends + PublicProfile+ income+ gender + distance + cust_age + numofposts, data = mydata )
stargazer( m2,
title="Regression Results", type="text",
column.labels=c("Model-2"),
df=FALSE, digits=2, star.cutoffs = c(0.05,0.01,0.001))
summary(m2,diagnostics = TRUE)
# model selection
negbin1b <- update(negbin1b, .~. + publicprofile)
## Model Fit ==> model fits
negbin1a <- glm.nb(purchase ~ 1, data=mydata)
anova(negbin1a,negbin1, test='Chisq')
#==========================================================
## NEGATIVE BINOMIAL REGRESSION
#==========================================================
negbin1 <- glm.nb(purchase ~ facebookvisit + income + gender + distance + cust_age + numofposts, data=mydata)
anova(negbin1a,negbin1, test='Chisq')
anova(negbin1a,negbin1, test='Chisq')
anova(negbin1b,negbin1, test='Chisq')
# model selection
#[publicprofile] ==> no better
negbin1b <- update(negbin1, .~. + publicprofile)
anova(negbin1b,negbin1, test='Chisq')
# model selection
#[publicprofile] ==> no better
negbin1b <- update(negbin1, .~. + publicprofile)
View(mydata)
# model selection
#[publicprofile] ==> no better
negbin1b <- update(negbin1, .~. + PublicProfile)
anova(negbin1b,negbin1, test='Chisq')
# [numoffriends]
negbin1c <- update(negbin1, .~. + numoffriends)
anova(negbin1c, negbin1, test='Chisq')
View(mydata)
#[numofposts]
negbin1d <- update(negbin1, .~. + numofposts)
anova(negbin1d, negbin1, test='Chisq')
#[numofposts + numoffriends]
poisson1d <- update(poisson1c, .~. + PublicProfile)
#[numofposts]
negbin1d <- update(negbin1, .~. + numofposts)
lrtest(negbin1d, negbin1)
#==========================================================
## NEGATIVE BINOMIAL REGRESSION
#==========================================================
negbin1 <- glm.nb(purchase ~ facebookvisit + income + gender + distance + cust_age , data=mydata)
# model selection
#[publicprofile] ==> no better
negbin1b <- update(negbin1, .~. + PublicProfile)
anova(negbin1b,negbin1, test='Chisq')
# [numoffriends]
negbin1c <- update(negbin1, .~. + numoffriends)
anova(negbin1c, negbin1, test='Chisq')
# [numofposts]
negbin1d <- update(negbin1, .~. + numofposts)
anova(negbin1d, negbin1, test='Chisq')
lrtest(negbin1d, negbin1)
# Set the directory
setwd("/Users/YingminWang/Desktop/OMIS\ 2392/Lab/Lab\ session\ 3 ")
# Set the directory
setwd("/Users/YingminWang/Desktop/OMIS\ 2392/Lab/Lab\ session\ 3")
# You should generally clear the working space at the start of every R session
rm(list = ls())
# install packages
#install.packages("ggeffects")
#install.packages("QuantPsyc")
install.packages("VIF")
install.packages("VIF")
mydata = read.csv("KaysJewelers.csv")
mydata = read.csv("KaysJewelers.csv")
dir()
# Set the directory
setwd("/Users/YingminWang/Desktop/OMIS\ 2392/Lab/Lab\ session\ 3 ")
# Set the directory
setwd("/Users/YingminWang/Desktop/OMIS\ 2392/Lab/Lab\ session\ 3 ")
# Set the directory
setwd("/Users/YingminWang/Desktop/OMIS\ 2392/Lab/Lab\ session\ 3")
# Set the directory
setwd("/Users/YingminWang/Desktop/OMIS\ 2392/Lab/Lab\ session\ 3")
mydata = read.csv("KaysJewelers.csv")
# Summary statistics
stargazer(mydata, type="text", median=TRUE, iqr=TRUE,digits=1, title="Descriptive Statistics")  # basic descriptive statistics
ggplot(mydata, aes(x=price)) + geom_histogram(colour="green") # we will use the original variable
ggplot(mydata, aes(x=log(price))) + geom_histogram(colour="green") # we will use the original variable
ggplot(mydata, aes(x=pastpurchase)) + geom_histogram(colour="green") # we will use the original variable
ggplot(mydata, aes(x=log(1+pastpurchase))) + geom_histogram(colour="green")
mydata$objectivequal=1-mydata$rejectrate
mydata$perceivedqual = mydata$survey4
mydata$female = ifelse(mydata$gender=="F",1,0)
df <- data.frame(mydata$perceivedqual,mydata$objectivequal,mydata$price,mydata$basketsize,mydata$pastpurchase,mydata$pastreturn,mydata$holiday,mydata$age,mydata$female,mydata$income )
cor(df) # Generates the correlation matrix
vifcor(df) #  VIF scores are less than 3, no indication of multicollinearity
probit1<- glm(return~perceivedqual+objectivequal+price+basketsize+pastpurchase+pastreturn+holiday+age+female+ income + factor(malltype), data=mydata, family=binomial(link="probit"))
stargazer(probit1,
title="Regression Results", type="text",
column.labels=c("Probit-1"),
df=FALSE, digits=3, star.cutoffs = c(0.05,0.01,0.001)) #  objective quality is not significant and perceived quality is significant and as expected
# without link, run logit
stargazer(probit1,
title="Regression Results", type="text",
column.labels=c("Probit-1"),
df=FALSE, digits=3, star.cutoffs = c(0.05,0.01,0.001)) #  objective quality is not significant and perceived quality is significant and as expected
probit1a <- glm(return~1, data=mydata, family=binomial(link="probit")) # This is the command to run a probit on null model
lrtest(probit1, probit1a) #We compare the null model to our model to determine the model fit. The p-value for the chi-square tet is  less than 0.001. Thus, our model fits significantly better than the null model.
## Measuring the predictive power of the probit
pred = predict(probit1, data=mydata, type="response")
head(pred)
head(pred)
return_prediction <- ifelse(pred >= 0.5,1,0)
return_prediction <- ifelse(pred >= 0.5,1,0)
misClasificError <- mean(return_prediction != mydata$return)
print(paste('Accuracy',1-misClasificError)) # the correct classification rate is 85.64%, which is quite good
# Check for heteroscedasticity
gqtest(probit1) # Significant Goldfeld-Quandt test does not indicate heteroscedasticity
bptest(probit1) # Significant Breusch-Pagan test indicates heteroscedasticity
HWrobstder <- sqrt(diag(vcovHC(probit1, type="HC1"))) # produces Huber-White robust standard errors
stargazer(probit1, probit1,
se=list(NULL, HWrobstder),
title="Regression Results", type="text",
column.labels=c("Normal SE", "HW-Robust SE"),
df=FALSE, digits=3, star.cutoffs = c(0.05,0.01,0.001))  # displays normal/HW robust  standard errors. objective quality is not significant and perceived quality is significant and as expected
## Adding Seasonality...
Aggregate_data <- aggregate(mydata[c("return")], by=list(mydata$month), mean)
ggplot(Aggregate_data, aes(x=Group.1, y=return)) + geom_line() + geom_point(size=3, colour="maroon") +
xlab("Month") + ylab("Return rate") # It seems there is monthly seasonal effect on returns
## Adding Seasonality...
Aggregate_data <- aggregate(mydata[c("return")], by=list(mydata$month), mean)
ggplot(Aggregate_data, aes(x=Group.1, y=return)) + geom_line() + geom_point(size=3, colour="maroon") +
xlab("Month") + ylab("Return rate") # It seems there is monthly seasonal effect on returns
Aggregate_data <- aggregate(mydata[c("return")], by=list(mydata$year), mean)
ggplot(Aggregate_data, aes(x=Group.1, y=return)) + geom_line() + geom_point(size=3, colour="maroon") +
xlab("Year") + ylab("Return rate") # It seems there is annual seasonal effect on returns
probit2<- glm(return~perceivedqual+objectivequal+price+basketsize+pastpurchase+pastreturn+holiday+age+female+ income + factor(malltype) + factor(month) + factor(year), data=mydata, family=binomial(link="probit"))
probit2a <- glm(return~1, data=mydata, family=binomial(link="probit")) # This is the command to run a logit on null model
lrtest(probit2, probit2a) #We compare the null model to our model to determine the model fit. The p-value for the chi-square tet is  less than 0.001. Thus, model probit-2 fits significantly better than the null model.
stargazer(probit1, probit2,
title="Regression Results", type="text",
column.labels=c("Probit-1", "Probit-2"),
df=FALSE, digits=2, star.cutoffs = c(0.05,0.01,0.001)) #  objective quality is not significant and perceived quality is significant and as expected
anova(probit1, probit2, test="Chisq") # It shows that seasonality variables improve model fit significantly
# Check for heteroscedasticity
gqtest(probit2) # Significant Goldfeld-Quandt test does not indicate heteroscedasticity
bptest(probit2) # Significant Breusch-Pagan test indicates heteroscedasticity
HWrobstder <- sqrt(diag(vcovHC(probit2, type="HC1"))) # produces Huber-White robust standard errors
stargazer(probit2, probit2,
se=list(NULL, HWrobstder),
title="Regression Results", type="text",
column.labels=c("Normal SE", "HW-Robust SE"),
df=FALSE, digits=3, star.cutoffs = c(0.05,0.01,0.001))  # displays normal/HW robust  standard errors. objective quality is not significant and perceived quality is significant and as expected
pred = predict(probit2, data=mydata, type="response")
return_prediction <- ifelse(pred >= 0.5,1,0)
misClasificError <- mean(return_prediction != mydata$return)
print(paste('Accuracy',1-misClasificError)) # the correct classification rate increased to 85.74%
# Reoperationalize perceived quality...
mydata$perceivedqual2 <- mydata$perceivedqual * mydata$price
# Reoperationalize perceived quality...
mydata$perceivedqual2 <- mydata$perceivedqual * mydata$price
probit3<- glm(return~perceivedqual2+objectivequal+price+basketsize+pastpurchase+pastreturn+holiday+age+female+ income + factor(malltype) + factor(month) + factor(year), data=mydata, family=binomial(link="probit"))
probit3a <- glm(return~1, data=mydata, family=binomial(link="probit")) # This is the command to run a logit on null model
lrtest(probit3, probit3a) #We compare the null model to our model to determine the model fit. The p-value for the chi-square tet is  less than 0.001. Thus, model probit-3 fits significantly better than the null model.
stargazer(probit2, probit3,
title="Regression Results", type="text",
column.labels=c("Probit-2", "Probit-3"),
df=FALSE, digits=2, star.cutoffs = c(0.05,0.01,0.001)) #  objective quality is not significant and perceived quality is significant and as expected
View(mydata)
AIC(probit2, probit3) # It shows that our initial operationalization of perceived quality is better
BIC(probit2, probit3)
View(probit2a)
mydata$competence <- (mydata$survey1 + mydata$survey2 + mydata$survey3)/3
probit4<- glm(return~perceivedqual+objectivequal+competence+price+basketsize+pastpurchase+pastreturn+holiday+age+female+ income + factor(malltype) + factor(month) + factor(year), data=mydata, family=binomial(link="probit"))
probit4a <- glm(return~1, data=mydata, family=binomial(link="probit")) # This is the command to run a logit on null model
lrtest(probit4, probit4a) #We compare the null model to our model to determine the model fit. The p-value for the chi-square test is  less than 0.001. Thus, model probit-4 fits significantly better than the null model.
stargazer(probit2, probit4,
title="Regression Results", type="text",
column.labels=c("Probit-2", "Probit-4"),
df=FALSE, digits=2, star.cutoffs = c(0.05,0.01,0.001)) # Objective quality is not significant, yet competence is significant and as expected
anova(probit2, probit4, test="Chisq") # It shows that competence significantly improves model fit
gqtest(probit4) # Significant Goldfeld-Quandt test does not indicate heteroscedasticity
bptest(probit4) # Significant Breusch-Pagan test indicates heteroscedasticity
HWrobstder <- sqrt(diag(vcovHC(probit4, type="HC1"))) # produces Huber-White robust standard errors
stargazer(probit4, probit4,
se=list(NULL, HWrobstder),
title="Regression Results", type="text",
column.labels=c("Normal SE", "HW-Robust SE"),
df=FALSE, digits=3, star.cutoffs = c(0.05,0.01,0.001))  # displays normal/HW robust  standard errors. objective quality is not significant and perceived quality is significant and as expected
pred = predict(probit4, data=mydata,type="response")
return_prediction <- ifelse(pred >= 0.5,1,0)
misClasificError <- mean(return_prediction != mydata$return)
print(paste('Accuracy',1-misClasificError)) # the correct classification rate increased to 91.12%
probit5<- glm(return~perceivedqual*competence+objectivequal+price+basketsize+pastpurchase+pastreturn+holiday+age+female+ income + factor(malltype) + factor(month) + factor(year), data=mydata, family=binomial(link="probit"))
probit5a <- glm(return~1, data=mydata, family=binomial(link="probit")) # This is the command to run a logit on null model
lrtest(probit5, probit5a) #We compare the null model to our model to determine the model fit. The p-value for the chi-square test is  less than 0.001. Thus, model probit-5 fits significantly better than the null model.
stargazer(probit4, probit5,
title="Regression Results", type="text",
column.labels=c("Probit-4", "Probit-5"),
df=FALSE, digits=2, star.cutoffs = c(0.05,0.01,0.001)) # The interaction term is significant
anova(probit4, probit5, test="Chisq") # It shows that the interaction term significantly improves model fit
gqtest(probit5) # Significant Goldfeld-Quandt test does not indicate heteroscedasticity
bptest(probit5) # Significant Breusch-Pagan test indicates heteroscedasticity
HWrobstder <- sqrt(diag(vcovHC(probit5, type="HC1"))) # produces Huber-White robust standard errors
stargazer(probit5, probit5,
se=list(NULL, HWrobstder),
title="Regression Results", type="text",
column.labels=c("Normal SE", "HW-Robust SE"),
df=FALSE, digits=3, star.cutoffs = c(0.05,0.01,0.001))  # displays normal/HW robust  standard errors. The interaction term is still significant
pred = predict(probit5, data=mydata,type="response")
return_prediction <- ifelse(pred >= 0.5,1,0)
misClasificError <- mean(return_prediction != mydata$return)
print(paste('Accuracy',1-misClasificError)) # the correct classification rate increased to 91.97%
# You should generally clear the working space at the start of every R session
rm(list = ls())
# Set the directory
setwd("C:/Users/nertekin/Dropbox/Santa Clara Teaching/OMIS 2392/Case Studies/Case Study-1")
# Set the directory
setwd(" /Users/YingminWang/Desktop/OMIS\ 2392/Case/case1-apple")
# Set the directory
setwd("/Users/YingminWang/Desktop/OMIS\ 2392/Case/case1-apple")
# Load libraries everytime you start a session
library(stargazer)
library(gdata)
library(ggplot2)
library(psych)
library(ggeffects)
library(QuantPsyc)
# turn off scientific notation except for big numbers.
options(scipen = 9)
# read in CSV
mydata = read.csv("AppleIT.csv")
# read in CSV
mydata = read.csv("AppleIT.csv")
# Plot the data
stargazer(mydata, type="text", median=TRUE, iqr=TRUE,digits=1, title="Descriptive Statistics")
# You should generally clear the working space at the start of every R session
rm(list = ls())
# Set the directory
setwd("/Users/YingminWang/Desktop/OMIS\ 2392/Case/case1-apple")
# Load libraries everytime you start a session
library(stargazer)
library(gdata)
library(ggplot2)
library(psych)
library(ggeffects)
library(QuantPsyc)
# turn off scientific notation except for big numbers.
options(scipen = 9)
# read in CSV
mydata = read.csv("AppleIT.csv")
# read in CSV
mydata = read.csv("AppleIT.csv")
# Plot the data
stargazer(mydata, type="text", median=TRUE, iqr=TRUE,digits=1, title="Descriptive Statistics")
ggplot(mydata, aes(x=completiontime)) + geom_histogram(colour="green")
ggplot(mydata, aes(x=completiontime)) + geom_histogram(colour="green")
ggplot(mydata, aes(x=log(completiontime))) + geom_histogram(colour="green") # use log transformed dependent variable
ggplot(mydata, aes(x=completiontime)) + geom_histogram(colour="green")
ggplot(mydata, aes(x=log(completiontime))) + geom_histogram(colour="green") # use log transformed dependent variable
ggplot(mydata, aes(x=scheduledtime)) + geom_histogram(colour="green")
ggplot(mydata, aes(x=scheduledtime)) + geom_histogram(colour="green")
ggplot(mydata, aes(x=log(scheduledtime))) + geom_histogram(colour="green") # use log transformed variable
ggplot(mydata, aes(x=log(scheduledtime))) + geom_histogram(colour="green") # use log transformed variable
ggplot(mydata, aes(x=groupsize)) + geom_histogram(colour="green", bins = 15)
ggplot(mydata, aes(x=log(groupsize))) + geom_histogram(colour="green", bins=15) # use raw variable
ggplot(mydata, aes(x=experience)) + geom_histogram(colour="green", bins = 15)
ggplot(mydata, aes(x=log(experience))) + geom_histogram(colour="green", bins = 15) # use raw variable
mydata$newit <- as.factor(mydata$newit)
ggplot(mydata, aes(x=newit, y=completiontime, fill=newit)) + geom_boxplot() +
xlab("Completion Time") + ylab("New IT") # # Does not visually show any difference between new IT system and old IT system
ggplot(mydata, aes(x=newit, y=completiontime, fill=newit)) + geom_boxplot() +
xlab("Completion Time") + ylab("New IT") # # Does not visually show any difference between new IT system and old IT system
ggplot(mydata[mydata$completiontime<=1500,], aes(x=newit, y=completiontime, fill=newit)) + geom_boxplot() +
xlab("Completion Time") + ylab("New IT") # We can demostrate only observations with completion time <= 1500. This way, we can zoom into the boxplot
# Basic OLS model
m1 <- lm(log(completiontime)~newit+log(scheduledtime)+training+groupsize+experience+gender,data=mydata)
stargazer(m1,
title="Regression Results", type="text",
column.labels=c("Model-1"),
df=FALSE, digits=2, star.cutoffs = c(0.05,0.01,0.001))
# Consider alternative models
altmodel1 <- lm(log(completiontime)~newit+log(scheduledtime)+training+groupsize+experience,data=mydata)
stargazer(m1, altmodel1,
title="Regression Results", type="text",
column.labels=c("Model-1", "Alt_Model-1"),
df=FALSE, digits=2, star.cutoffs = c(0.05,0.01,0.001))
anova(m1, altmodel1, test="Chisq") # altmodel1 is better than m1.
AIC(m1,altmodel1) # This is the wrong test
AIC(m1,altmodel1) # This is the wrong test
AIC(m1,altmodel1) # This is the wrong test
BIC(m1,altmodel1) # This is the wrong test
altmodel2 <- lm(log(completiontime)~newit+scheduledtime+training+groupsize+experience,data=mydata)
altmodel2 <- lm(log(completiontime)~newit+scheduledtime+training+groupsize+experience,data=mydata)
stargazer(altmodel1, altmodel2,
title="Regression Results", type="text",
column.labels=c( "AltModel-1", "AltModel-2"),
df=FALSE, digits=2, star.cutoffs = c(0.05,0.01,0.001))
AIC(altmodel1,altmodel2)
BIC(altmodel1,altmodel2) # altModel-1 is better than altmodel-2
altmodel3 <- lm(log(completiontime)~newit+log(scheduledtime)+training+groupsize+I(groupsize^2)+experience,data=mydata)
stargazer(altmodel1, altmodel3,
title="Regression Results", type="text",
column.labels=c( "AltModel-1", "AltModel-3"),
df=FALSE, digits=2, star.cutoffs = c(0.05,0.01,0.001))
anova(altmodel1, altmodel3, test="Chisq") # altmodel-1 is better than altmodel-3.
altmodel4 <- lm(log(completiontime)~newit+log(scheduledtime)+training+groupsize+experience+I(experience^2),data=mydata)
stargazer(altmodel1, altmodel4,
title="Regression Results", type="text",
column.labels=c( "AltModel-1", "AltModel-4"),
df=FALSE, digits=2, star.cutoffs = c(0.05,0.01,0.001))
anova(altmodel1, altmodel4, test="Chisq") # altmodel-1 is better than altmodel-4.
res2 <- lm(log(completiontime)~newit*training+log(scheduledtime)+groupsize+experience,data=mydata)
res2 <- lm(log(completiontime)~newit*training+log(scheduledtime)+groupsize+experience,data=mydata)
stargazer(altmodel1, res2,
title="Regression Results", type="text",
column.labels=c( "AltModel-1", "IntModel-4"),
df=FALSE, digits=2, star.cutoffs = c(0.05,0.01,0.001))
anova(altmodel1, res2, test="Chisq") # the interaction model is better than altmodel-1
meffects <- ggpredict(res2, terms=c("newit", "training")) # generates a tidy data frame
ggplot(meffects,aes(x, predicted, colour=group)) + geom_line(size=1.3) +
xlab("New IT") + ylab("Predicted log(Completion Time)") +
labs(colour="Training") +
scale_colour_discrete(labels=c("No", "Yes")) +
scale_x_continuous(breaks=c(0,1), labels=c("Old IT", "New IT")) +
theme(axis.title.x=element_blank())
mydata2 <- subset(mydata, training == 1) # generates a subset of data using observations only with training value ==1
res3 = lm(log(completiontime)~newit+log(scheduledtime)+groupsize+experience+gender,data=mydata2)
stargazer(res3,
title="Regression Results", type="text",
column.labels=c( "Trained"),
df=FALSE, digits=2, star.cutoffs = c(0.05,0.01,0.001))
mydata3 <- subset(mydata, training == 0) # generates a subset of data using observations only with training value ==0
res4 = lm(log(completiontime)~newit+log(scheduledtime)+groupsize+experience+gender,data=mydata3)
stargazer(res3, res4,
title="Regression Results", type="text",
column.labels=c( "Trained", "Untrained"),
df=FALSE, digits=2, star.cutoffs = c(0.05,0.01,0.001))
#==================================================
# set up
#==================================================
rm(list=ls())
setwd('/Users/YingminWang/Desktop/OMIS\ 2392/Case/case4-amazon')
dir()
mydata <- read.csv('Amazon.csv')
#===================================================
# Build up model
#===================================================
#0. select variable
m0 <- lm(lnpurchase ~ facebookvisit + income + gender + distance + cust_age + numoffriends + numofposts + PublicProfile, data=mydata)
m1 <- lm(lnpurchase ~ facebookvisit + income+ gender + distance + cust_age + numofposts, data=mydata)
#===================================================
# Poisson Regression
#===================================================
poisson1 <- glm(purchase ~ facebookvisit + income + gender + distance + cust_age, family='poisson', data=mydata)
# add [numofposts]  ==> poisson1c better
poisson1c <- update(poisson1, .~. +numofposts)
#==========================================================
## NEGATIVE BINOMIAL REGRESSION
#==========================================================
negbin1 <- glm.nb(purchase ~ facebookvisit + income + gender + distance + cust_age , data=mydata)
# IRRs          ?? The same as Poission?
stargazer(negbin1,
apply.coef=exp, t.auto=F, p.auto=F,
title='Regression Results', type='text',
column.labels = 'IRRs',
df=FALSE, digits=4, star.cutoffs = c(0.05,0.01,0.001))
##  Visualize the output
meffects2 <- ggpredict(negbin1, terms=c("facebookvisit"))
ggplot(meffects2,aes(x, predicted)) + geom_point(size=3, colour="maroon") + geom_line() +
xlab("Facebook Visit") + ylab("Predicted purchase") +
scale_x_continuous(breaks=c(0,1), labels=c("with facebook visit", "without facebook visit"))
ggplot(meffects2,aes(x, predicted)) + geom_point(size=3, colour="maroon") + geom_line() +
xlab("Facebook Visit") + ylab("Predicted purchase") +
scale_x_continuous(breaks=c(0,1), labels=c("with facebook visit", "without facebook visit"))
ggplot(meffects2,aes(x, predicted)) + geom_point(size=3, colour="maroon") + geom_line() +
xlab("Facebook Visit") + ylab("Predicted purchase") +
scale_x_continuous(breaks=c(0,1), labels=c("No", "Yes"))
stargazer(negbin1, negbin1,
se=list(NULL, HWrobstder),
title="Regression Results", type="text",
column.labels=c("Normal SE", "HW-Robust SE"),
df=FALSE, digits=4, star.cutoffs = c(0.05,0.01,0.001))
HWrobstder2 <- sqrt(diag(vcovHC(negbin1,, type="HC1")))
stargazer(negbin1, negbin1,
se=list(NULL, HWrobstder),
title="Regression Results", type="text",
column.labels=c("Normal SE", "HW-Robust SE"),
df=FALSE, digits=4, star.cutoffs = c(0.05,0.01,0.001))
HWrobstder2 <- sqrt(diag(vcovHC(negbin1,, type="HC1")))
stargazer(negbin1, negbin1,
se=list(NULL, HWrobstder),
title="Regression Results", type="text",
column.labels=c("Normal SE", "HW-Robust SE"),
df=FALSE, digits=4, star.cutoffs = c(0.05,0.01,0.001))
stargazer(negbin1, negbin1,
se=list(NULL, HWrobstder2),
title="Regression Results", type="text",
column.labels=c("Normal SE", "HW-Robust SE"),
df=FALSE, digits=4, star.cutoffs = c(0.05,0.01,0.001))
newdata2 <- data.frame(income=mean(mydata$income), gender=mean(mydata$gender),
distance = mean(mydata$distance), cust_age=mean(mydata$cust_age),
numofposts=mean(mydata$numofposts), facebookvisit = c(0,1))
newdata2 <- data.frame(income=mean(mydata$income), gender=mean(mydata$gender),
distance = mean(mydata$distance), cust_age=mean(mydata$cust_age),
numofposts=mean(mydata$numofposts), facebookvisit = c(0,1))
newdata2 <- data.frame(income=mean(mydata$income), gender=mean(mydata$gender),
distance = mean(mydata$distance), cust_age=mean(mydata$cust_age),
numofposts=mean(mydata$numofposts), facebookvisit = c(0,1))
newdata2$predicted_purchase <- predict(negbin1, newdata, type="response")
newdata2$predicted_purchase <- predict(negbin1, newdata2, type="response")
newdata2
confint(negbin1, facebookvisit)
confint(negbin1, 'facebookvisit')
#==========================================================
## Compare with linear model
#==========================================================
# different
m1
m1 <- lm(lnpurchase ~ facebookvisit + income+ gender + distance + cust_age + numofposts, data=mydata)
# ==>  use log
mydata$lnpurchase <- log(mydata$purchase)
m1 <- lm(lnpurchase ~ facebookvisit + income+ gender + distance + cust_age + numofposts, data=mydata)
#==========================================================
## Compare with linear model
#==========================================================
# different
m1
stargazer(m1,negbin1,
title="Regression Results", type="text",
column.labels=c("Linear Regression", "Negative Binomial"),
df=FALSE, digits=2, star.cutoffs = c(0.05,0.01,0.001))
confint(negbin1, 'facebookvisit')
pred<-predict(negbin1)
res=resid(model1)
## Heteroscedasticity  ==> yes!!  ?? visually?
df <- mydata[c('purchase', 'facebookvisit', 'income', 'gender', 'distance', 'cust_age')]
pred<-predict(negbin1)
res=resid(model1)
res=resid(negbin1)
ggplot(df, aes(y=res, x=pred)) + geom_point(size=2.5)
ggplot(df, aes(y=res, x=pred)) + geom_point(size=1)
## Heteroscedasticity  ==> yes!!  ?? visually?
df <- mydata[c('lnpurchase', 'facebookvisit', 'income', 'gender', 'distance', 'cust_age')]
ggplot(df, aes(y=res, x=pred)) + geom_point(size=1)
## Heteroscedasticity  ==> yes!!  ?? visually?
df <- mydata[c('lnpurchase', 'facebookvisit', 'income', 'gender', 'distance', 'cust_age')]
ggplot(df, aes(y=res, x=pred)) + geom_point(size=1)
head(mydata)
mydata <- read.csv('Amazon.csv')
stargazer(mydata, type='text', title='descriptive analysis', digits=2, median=TRUE, iqr=TRUE)
